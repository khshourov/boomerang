# Task 4.3: Implement Connection Pooling for Outbound Callbacks [COMPLETED]

## Objective
Efficiently manage and reuse outbound connections for TCP and gRPC callbacks to reduce latency and resource overhead.

## Requirements
- **TCP Connection Pooling:** Implement a connection pool for `TcpCallbackHandler` using Netty's `FixedChannelPool` or `SimpleChannelPool`.
- **gRPC Channel Pooling:** Enhance `GrpcCallbackHandler` with a managed pool that supports a maximum number of open channels and an idle timeout/eviction policy.
- **Configurability:** Expose pool sizes and timeouts in `ServerConfig.java` and `boomerang-server.properties`.
- **Resource Management:** Ensure all pools are properly shut down when the server stops.
- **Thread Safety:** Ensure the pooling logic is thread-safe for high-concurrency callback dispatching.

## Proposed Changes
### `io.boomerang.config.ServerConfig`
- Add methods:
    - `getCallbackTcpPoolMaxConnections()` (default: 50)
    - `getCallbackGrpcPoolMaxChannels()` (default: 100)
    - `getCallbackGrpcIdleTimeoutMs()` (default: 60000)

### `io.boomerang.server.callback.TcpCallbackHandler`
- Replace the per-call connection logic with a `ChannelPoolMap`.
- Use `FixedChannelPool` for each endpoint to limit concurrent connections.

### `io.boomerang.server.callback.GrpcCallbackHandler`
- Replace the `ConcurrentHashMap` with a more robust cache-like structure (e.g., using Caffeine or a simple LRU with TTL) if necessary, or manually manage idle timeouts.
- Implement channel eviction to prevent memory leaks with many unique endpoints.

## Verification Plan
- **Unit/Integration Tests:**
    - Verify that `TcpCallbackHandler` reuses connections to the same endpoint.
    - Verify that `TcpCallbackHandler` respects the maximum connection limit.
    - Verify that `GrpcCallbackHandler` evicts idle channels.
- **Performance Benchmarking (Optional but recommended):**
    - Compare callback throughput and latency before and after pooling.
